{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from environment import *\n",
    "from mix_state_env import MixStateEnv\n",
    "from config import *\n",
    "import copy\n",
    "from MyGlobal import MyGlobals\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "state_size = NUM_STATE\n",
    "action_size = NUM_ACTION\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(actor, critic, num_iters, num_episodes, duration, gamma, env):\n",
    "    optimizerA = optim.Adam(actor.parameters())\n",
    "    optimizerC = optim.Adam(critic.parameters())\n",
    "    exploit_rate_files = open(\n",
    "        RESULT_DIR + MyGlobals.folder_name + \"exploit_rate.csv\", \"w\")\n",
    "    exploit_rate_files.write('1')\n",
    "    for i in range(2, NUM_ACTION + 1):\n",
    "        exploit_rate_files.write(',' + str(i))\n",
    "    exploit_rate_files.write('\\n')\n",
    "\n",
    "    for iter in range(num_iters):\n",
    "        env.replay()\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            count_exploit = [0] * NUM_ACTION\n",
    "\n",
    "            while not done:\n",
    "                log_probs = []\n",
    "                values = []\n",
    "                rewards = []\n",
    "                masks = []\n",
    "                for i in count():\n",
    "                    state = torch.FloatTensor(state).to(device)\n",
    "                    dist, value = actor(state), critic(state)\n",
    "\n",
    "                    action = dist.sample()\n",
    "                    prob_dist = dist.probs\n",
    "                    # print(type(prob_dist))\n",
    "                    # print(prob_dist)\n",
    "                    # print(action)\n",
    "                    # print(torch.topk(prob_dist.flatten(), NUM_ACTION).indices.tolist())\n",
    "                    # print(torch.topk(prob_dist.flatten(), NUM_ACTION).indices.tolist().index(action))\n",
    "                    # print(torch.topk(\n",
    "                    #     prob_dist.flatten(), NUM_ACTION))\n",
    "                    # assert 2 == 3\n",
    "                    count_exploit[torch.topk(\n",
    "                        prob_dist.flatten(), NUM_ACTION).indices.tolist().index(action)] += 1\n",
    "                    # if action == dist.probs.argmax():\n",
    "                    #     count_exploit += 1\n",
    "                    next_state, reward, done, _ = env.step(\n",
    "                        action.cpu().numpy())\n",
    "\n",
    "                    log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "\n",
    "                    log_probs.append(log_prob)\n",
    "                    values.append(value)\n",
    "                    rewards.append(torch.tensor(\n",
    "                        [reward], dtype=torch.float, device=device))\n",
    "                    masks.append(torch.tensor(\n",
    "                        [1-done], dtype=torch.float, device=device))\n",
    "\n",
    "                    state = next_state\n",
    "\n",
    "                    if done:\n",
    "                        if (env.old_avg_reward < -1500):\n",
    "                            return\n",
    "                        print(i)\n",
    "                        print('Episode: {}, Score: {}'.format(\n",
    "                            episode, env.old_avg_reward))\n",
    "\n",
    "                        # print(dist.probs)\n",
    "                        #print('Iteration: {}, Score: {}'.format(episode, i))\n",
    "                        break\n",
    "\n",
    "                    if (i > duration):\n",
    "                        break\n",
    "\n",
    "                next_state = torch.FloatTensor(next_state).to(device)\n",
    "                next_value = critic(next_state)\n",
    "                returns = compute_returns(\n",
    "                    next_value, rewards, masks, gamma=gamma)\n",
    "\n",
    "                log_probs_cat = torch.cat(log_probs)\n",
    "                returns_cat = torch.cat(returns).detach()\n",
    "                values_cat = torch.cat(values)\n",
    "\n",
    "                advantage = returns_cat - values_cat\n",
    "\n",
    "                actor_loss = -(log_probs_cat * advantage.detach()).mean()\n",
    "                critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "                optimizerA.zero_grad()\n",
    "                optimizerC.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                critic_loss.backward()\n",
    "                optimizerA.step()\n",
    "                optimizerC.step()\n",
    "\n",
    "            tempstr = ','.join([str(elem) for elem in count_exploit])\n",
    "            exploit_rate_files.write(tempstr+\"\\n\")\n",
    "            # exploit_rate_files.write('{}\\n'.format(count_exploit))\n",
    "            print(tempstr)\n",
    "\n",
    "    exploit_rate_files.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(actor, critic, num_episodes, env):\n",
    "    if (env.old_avg_reward < -1500):\n",
    "        return\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.probs.argmax()\n",
    "            next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        print('Test Episode: {}, Score: {}'.format(episode, env.old_avg_reward))\n",
    "\n",
    "# MyGlobals.folder_name = \"Actor_Critic/dur10_g_0_99/\" + '1' +'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAC(i, dur, gamma):\n",
    "    # MyGlobals.folder_name = \"Actor_Critic_800_30s/dur\" + str(dur) + \"/\" + str(i) +'/'\n",
    "    MyGlobals.folder_name = f\"test/gamma{gamma}/dur{dur}/{i}/\"\n",
    "    env = MixStateEnv()\n",
    "    env.seed(123)\n",
    "    actor = Actor(state_size, action_size).to(device)\n",
    "    critic = Critic(state_size, action_size).to(device)\n",
    "    # 9, 10, 12, 15, 20, 22 \n",
    "    train(actor, critic, num_iters=9, num_episodes=121,\n",
    "          duration=dur, gamma=gamma, env=env)\n",
    "    test(actor, critic, num_episodes=31, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'd:\\\\Lab\\\\RL\\\\AODAI_RL\\\\result/result3/test/gamma0.99/dur30/1/'\n",
      "[186, 131, 79, 68, 153, 92, 91]\n",
      "31\n",
      "Episode: 0, Score: -1.4989184282635148\n",
      "194,155,134,100,87,74,56\n",
      "[437, 317, 4, 9, 30, 1, 2]\n",
      "31\n",
      "Episode: 1, Score: -23.46053028076649\n",
      "616,141,30,5,7,0,1\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 2, Score: -159.69988462197796\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 3, Score: -348.8521214569705\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 4, Score: -538.7966152687542\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 5, Score: -728.9677755380002\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 6, Score: -918.4326162781306\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 7, Score: -1107.4870741512743\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 8, Score: -1297.4434487570393\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n",
      "31\n",
      "Episode: 9, Score: -1487.5866542185304\n",
      "800,0,0,0,0,0,0\n",
      "[0, 800, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "runAC(1, 30, 0.99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
