{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "from environment import *\n",
    "from mix_state_env import MixStateEnv\n",
    "from config import *\n",
    "import copy\n",
    "from MyGlobal import MyGlobals\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "import math\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import time\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "file = open(\"myfile.txt\", \"w\")\n",
    "\n",
    "\n",
    "# Configure the XGBoost model with the custom Huber loss function\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    residual = y_true - y_pred\n",
    "    if abs(residual) <= delta:\n",
    "        return 0.5 * residual**2\n",
    "    else:\n",
    "        return delta * (abs(residual) - 0.5 * delta)\n",
    "def huber_loss_gradient(y_true, y_pred, delta):\n",
    "    residual = y_true - y_pred\n",
    "    if abs(residual) <= delta:\n",
    "        return residual\n",
    "    else:\n",
    "        return delta * (residual / abs(residual))\n",
    "\n",
    "def huber_loss_hessian(y_true, y_pred, delta):\n",
    "    residual = y_true - y_pred\n",
    "    if abs(residual) <= delta:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return delta / abs(residual)\n",
    "def custom_obj(y_true, y_pred):\n",
    "    obj=lambda y_true, y_pred: huber_loss(y_true, y_pred, delta),\n",
    "    grad=lambda y_true, y_pred: huber_loss_gradient(y_true, y_pred, delta),\n",
    "    hess=lambda y_true, y_pred: huber_loss_hessian(y_true, y_pred, delta)\n",
    "delta=1\n",
    "\n",
    "plt.ion()\n",
    "# def is_max_tree_reached(multioutput_model):\n",
    "#     # Get the maximum number of trees allowed\n",
    "#     max_trees = multioutput_model.estimators_[0].get_params()['n_estimators']\n",
    "#     # Check if the current number of trees in each output regressor is equal to the maximum\n",
    "#     for estimator in multioutput_model.estimators_:\n",
    "#         current_trees = estimator.get_booster().best_ntree_limit\n",
    "#         if current_trees != max_trees:\n",
    "#             return False\n",
    "    \n",
    "    # return True\n",
    "# if gpu is to be used\n",
    "device = device = torch.device(\"cpu\")\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity) -> None:\n",
    "        self.memory=deque([],maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition',('state', 'action', 'next_state', 'reward'))\n",
    "    def push(self, *args):\n",
    "        self.memory.append(self.Transition(*args))\n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "class DQNnet(nn.Module):\n",
    "    def __init__(self,n_observations,n_actions):\n",
    "        super(DQNnet,self).__init__()\n",
    "        self.layer1=nn.Linear(n_observations,128)\n",
    "        self.layer2=nn.Linear(128,128)\n",
    "        self.layer3=nn.Linear(128,n_actions)\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.layer1(x))\n",
    "        x=F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "class DQNAgent:\n",
    "    def __init__(self) :\n",
    "        self.optimize=0\n",
    "        self.max=False\n",
    "        self.env=MixStateEnv()\n",
    "        self.env.seed(123)\n",
    "        self.batch_size=2000\n",
    "        self.eps_start=0.9\n",
    "        self.eps_end=0.1\n",
    "        self.eps_decay=1000\n",
    "        self.tau=0.1\n",
    "        self.lr=1e-4\n",
    "        self.n_actions=NUM_ACTION\n",
    "        self.n_observations=NUM_STATE\n",
    "        self.isFit=False\n",
    "        self.memory=ReplayMemory(7000)\n",
    "        self.episode_durations = []\n",
    "        self.stepdone=0\n",
    "        self.delta = 1.0  # Threshold parameter\n",
    "    def huber_loss(self,y_true, y_pred, delta):\n",
    "        residual = torch.from_numpy(y_true).gather(1, self.action_batch)-torch.from_numpy(y_pred).gather(1, self.action_batch)\n",
    "        residual=residual.numpy()\n",
    "        if abs(residual) <= delta:\n",
    "            return 0.5 * residual**2\n",
    "        else:\n",
    "            return delta * (abs(residual) - 0.5 * delta)\n",
    "    def huber_loss_gradient(self,y_true, y_pred):\n",
    "        residual = torch.from_numpy(y_true).gather(1, self.action_batch)-torch.from_numpy(y_pred).gather(1, self.action_batch)\n",
    "        residual=residual.numpy()\n",
    "        if abs(residual) <= self.delta:\n",
    "            return residual\n",
    "        else:\n",
    "            return self.delta * (residual / abs(residual))\n",
    "\n",
    "    def huber_loss_hessian(self,y_true, y_pred):\n",
    "        residual = torch.from_numpy(y_true).gather(1, self.action_batch)-torch.from_numpy(y_pred).gather(1, self.action_batch)\n",
    "        residual=residual.numpy()\n",
    "        if abs(residual) <= self.delta:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return self.delta / abs(residual)\n",
    "    def custom_obj(self,y_true, y_pred):\n",
    "        obj=lambda y_true, y_pred: self.huber_loss(y_true, y_pred),\n",
    "        grad=lambda y_true, y_pred: self.huber_loss_gradient(y_true, y_pred),\n",
    "        hess=lambda y_true, y_pred: self.huber_loss_hessian(y_true, y_pred)\n",
    "# Create the custom loss function object\n",
    " \n",
    "    def select_action(self,state):\n",
    "            sample=random.random()\n",
    "            eps_threshold=self.eps_end+(self.eps_start-self.eps_end)*math.exp(-1.*self.stepdone/self.eps_decay)\n",
    "            self.stepdone+=0.005\n",
    "            if sample > eps_threshold:\n",
    "                with torch.no_grad():\n",
    "                    if self.isFit==True:\n",
    "                        q_value= self.model1.predict(state.reshape(1, -1))\n",
    "                    else: \n",
    "                        q_value=np.zeros(self.n_actions)\n",
    "                    return torch.tensor(np.argmax(q_value[0])).view(1, 1)\n",
    "            else:\n",
    "                return torch.tensor([[random.randint(0,NUM_ACTION-1)]], device=device, dtype=torch.long)\n",
    "    def optimize_model(self,upgrade,reset):\n",
    "            if len(self.memory) < self.batch_size:\n",
    "                return\n",
    "                \n",
    "            transitions = self.memory.sample(self.batch_size)\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            \n",
    "            # non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "            #                                     batch.next_state)), device=device, dtype=torch.bool)\n",
    "            # non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "            #                                             if s is not None])\n",
    "            batch_state=[]\n",
    "            for i in batch.state :\n",
    "                batch_state.append(i.view(1,14))\n",
    "            batch_next_state=[]\n",
    "            for i in batch.next_state :\n",
    "                batch_next_state.append(i.view(1,14))\n",
    "            next_state_batch = torch.cat(batch_next_state)\n",
    "            state_batch = torch.cat(batch_state)\n",
    "            self.action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "            index_tensor = torch.arange(self.action_batch.size(0)).reshape(-1, 1)\n",
    "            if self.isFit:\n",
    "                state_value1=torch.from_numpy(self.model1.predict(state_batch))\n",
    "                state_value2=torch.from_numpy(self.model2.predict(state_batch))\n",
    "            else:\n",
    "                state_value1=state_value2=torch.zeros((self.batch_size,self.n_actions))\n",
    "            # state_action_values = self.model1(state_batch).gather(1, action_batch)\n",
    "            # values=torch.zeros(self.batch_size,self.n_actions, device=device)\n",
    "            # values[index][action_batch]=state_value[index][action_batch]\n",
    "            indices=torch.cat((index_tensor,self.action_batch),dim=1)\n",
    "            if self.isFit:\n",
    "                next_state_values2 =torch.from_numpy(np.array(self.model1.predict(next_state_batch).max(1)[0]))\n",
    "                next_state_values1  =torch.from_numpy(np.array(self.model2.predict(next_state_batch).max(1)[0]))\n",
    "            else:\n",
    "                next_state_values1=next_state_values2=reward_batch.float()\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values1 = (next_state_values1 * self.gamma) + reward_batch\n",
    "            expected_state_action_values2 = (next_state_values2 * self.gamma) + reward_batch\n",
    "            torch.index_put_(state_value1, tuple(indices.t()), expected_state_action_values1.float())\n",
    "            torch.index_put_(state_value2, tuple(indices.t()), expected_state_action_values2.float())\n",
    "            if upgrade:\n",
    "                joblib.dump(self.model1.estimator, 'initial_model.pkl')\n",
    "                self.loaded_xgb_regressor = joblib.load('initial_model.pkl')\n",
    "                self.model2=MultiOutputRegressor(self.loaded_xgb_regressor)\n",
    "                if reset:\n",
    "                    self.model1=MultiOutputRegressor(XGBRegressor(learning_rate=self.learning_rate,n_estimators=300,max_depth=4,n_jobs=1,tree_method='gpu_hist'))\n",
    "                self.model1.estimator.n_estimators += self.tree\n",
    "                # self.model2=MultiOutputRegressor(XGBRegressor(learning_rate=self.learning_rate,n_estimators=200,max_depth=10,n_jobs=1,tree_method='gpu_hist'))\n",
    "            self.model1.fit(state_batch, state_value2)\n",
    "            self.model2.fit(state_batch, state_value1)\n",
    "            self.isFit = True   \n",
    "            # print(f\"optimze_time:{self.end_time-self.start_time},calculate:{self.calculate-self.start_time};times:{self.optimize}\")\n",
    "    def train(self,num_iters,num_episodes,duration,tree,learning_rate,gamma):\n",
    "            self.learning_rate=learning_rate\n",
    "            self.gamma=gamma\n",
    "            self.tree=tree\n",
    "            self.num_iters=num_iters\n",
    "            self.num_episodes=num_episodes\n",
    "            self.model1=MultiOutputRegressor(XGBRegressor(learning_rate=self.learning_rate,n_estimators=300,max_depth=5,n_jobs=1,tree_method='gpu_hist',gpu_id=0))\n",
    "            self.model2=MultiOutputRegressor(XGBRegressor(learning_rate=self.learning_rate,n_estimators=300,max_depth=5,n_jobs=1,tree_method='gpu_hist',gpu_id=0))\n",
    "            self.time_start=time.time()\n",
    "            for iter in range(self.num_iters):\n",
    "                self.env.replay()\n",
    "                for episode in range(self.num_episodes):\n",
    "                    state = self.env.reset()\n",
    "                    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    done=False\n",
    "                    while not done:\n",
    "                        for i in count():\n",
    "                            state = torch.FloatTensor(state).to(device)\n",
    "                            action = self.select_action(state)\n",
    "                            action1=action.item()\n",
    "                            next_state, reward, done, _= self.env.step(np.array(action1))\n",
    "                            reward = torch.tensor([reward], device=device)\n",
    "                            next_state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                            self.memory.push(state, action, next_state, reward)\n",
    "                            state = next_state\n",
    "                            if done:\n",
    "                                if (self.env.old_avg_reward < -1500):\n",
    "                                    return\n",
    "                                print('Episode: {}, Score: {}'.format(\n",
    "                                    episode, self.env.old_avg_reward))\n",
    "\n",
    "                                break\n",
    "                            \n",
    "                                \n",
    "                            if (i > duration):\n",
    "                                break\n",
    "                    if done:\n",
    "                        if episode==0: \n",
    "                                self.optimize_model(upgrade=False,reset=False)\n",
    "                        else:  \n",
    "                            if  done and episode % 60==0:\n",
    "                            # if self.isFit and (not self.is_max_tree_reached1() or not self.is_max_tree_reached2()):\n",
    "                                if iter!=0 and iter%2==0:\n",
    "                                    self.optimize_model(upgrade=False,reset=False)\n",
    "                                else:\n",
    "                                    self.optimize_model(upgrade=True,reset=False)\n",
    "                            else:\n",
    "                                self.optimize_model(upgrade=False,reset=False)\n",
    "\n",
    "                    # if len(self.memory) < 2000:\n",
    "                    #     self.batch_size=len(self.memory)//3\n",
    "                    # else:\n",
    "                    #     self.batch_size=2000\n",
    "            # self.time_end=time.time()\n",
    "            # print(f\"time:{self.time_end-self.time_start}\")\n",
    "            \n",
    "    def test(self,num_episodes):\n",
    "        if (self.env.old_avg_reward < -1500):\n",
    "            return\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                action = self.select_action(state)\n",
    "                action1=action.item()\n",
    "                next_state, reward, done, _= self.env.step(np.array(action1))\n",
    "                state = next_state\n",
    "\n",
    "            print('Test Episode: {}, Score: {}'.format(episode, self.env.old_avg_reward))\n",
    "            file.write('\\nTest Episode: {}, Score: {}'.format(episode, self.env.old_avg_reward))\n",
    "        print(self.num_iters,self.num_episodes,self.tree,self.learning_rate,self.gamma)\n",
    "        file.write('\\niter: {}, episode: {}, tree{},learning_rate:{},gamma:{}'.format(self.num_iters,self.num_episodes,self.tree,self.learning_rate,self.gamma))\n",
    "    def runAC(self, iters,episode,dur,tree,learning_rate,gamma):\n",
    "        self.train(num_iters=iters, num_episodes=episode,\n",
    "            duration=dur,tree= tree,learning_rate=learning_rate,gamma=gamma)\n",
    "        self.test( num_episodes=31, )\n",
    "Agent=DQNAgent()\n",
    "Agent.runAC(iters=7,episode=120,dur=31,tree=60,learning_rate=0.1,gamma=0.99)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
